from typing import List
import random
import io
import base64
import time
import pandas as pd
import streamlit as st
from audio_recorder_streamlit import audio_recorder
from openai import OpenAI
from langchain_openai import ChatOpenAI
from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate
from langchain.schema import StrOutputParser, AIMessage, HumanMessage, SystemMessage
from langchain_core.output_parsers import JsonOutputParser
from langchain_core.pydantic_v1 import BaseModel, Field
import os
os.environ["OPENAI_API_KEY"] = "Your API Key"

# Set page configuration for wide layout
st.set_page_config(layout="wide")

if "curr_page" not in st.session_state:
    st.session_state["curr_page"] = "home"
    st.session_state["curr_topic"] = "home"

if "prev_audio_bytes" not in st.session_state:
    st.session_state.prev_audio_bytes = None

if "exam_context" not in st.session_state:
    st.session_state.exam_context = {}


client = OpenAI()


def autoplay_audio(file_path: str):
    with open(file_path, "rb") as f:
        data = f.read()
        b64 = base64.b64encode(data).decode()
        md = f"""
            <audio controls autoplay="true">
            <source src="data:audio/mp3;base64,{b64}" type="audio/mp3">
            </audio>
            """
        st.markdown(
            md,
            unsafe_allow_html=True,
        )

def recognize_speech():
    user_input = ""
    # ì§ˆë¬¸ì— ë‹µí•˜ê¸°
    audio_bytes = audio_recorder("talk", pause_threshold=3.0,)
    if audio_bytes == st.session_state.prev_audio_bytes:
        audio_bytes = None
    st.session_state.prev_audio_bytes = audio_bytes

    try:
        if audio_bytes:
            with st.spinner(" Voice recognition in progress..."):
                with open("./tmp_audio.wav", "wb") as f:
                    f.write(audio_bytes)

                with open("./tmp_audio.wav", "rb") as f: 
                    transcript = client.audio.transcriptions.create(
                        model="whisper-1",
                        file=f,
                        language="en"
                    )
                    user_input = transcript.text
    except Exception as e:
        print(e)
        pass
    return user_input


# Assuming you have a dictionary that holds your data like below:
speaking_topic_to_topic_info_map = {
    'speaking__listen_and_answer': {'display_name': 'Listen and answer the questions', 'emoji': 'ğŸ’­'},
    'speaking__express_an_opinion': {'display_name': 'Express your opinion', 'emoji': 'ğŸ—£ï¸'},
    'speaking__debate': {'display_name': 'Discussion', 'emoji': 'ğŸ‘©â€'},
    'speaking__describe_img': {'display_name': 'Describing a photo', 'emoji': 'ğŸï¸'},
    'speaking__describe_charts': {'display_name': 'Analyze information from a chart', 'emoji': 'ğŸ“Š'},
}

writing_topic_to_topic_info_map = {
    'writing__dictation': {'display_name': 'Creating a wrting test format', 'emoji': 'âœï¸'},
    'writing__responding_to_an_email': {'display_name': 'Replying to an email', 'emoji': 'âœ‰ï¸'},
    'writing__summarization': {'display_name': 'Summarizing the contents of the given prompt', 'emoji': 'âœï¸'},
    'writing__writing_opinion': {'display_name': 'Writing your opinion', 'emoji': 'ğŸ“'},
}


def go_to_topic(topic):
    st.session_state["curr_page"] = topic
    st.session_state["curr_topic"] = topic

def go_to_result():
    st.session_state["curr_page"] = "result"

# Create a function to display each topic in the grid
def display_topic(topic, topic_info, key):
    with st.container(border=True):
        st.write(f"{topic_info['emoji']} **{topic_info['display_name']}**")
        st.button("Start", key=f"start_{topic}_{key}", on_click=go_to_topic, kwargs=dict(topic=topic))


con = st.container()
if st.session_state["curr_page"] == "home":
    with con:
        st.title("Speaking & Writing Test")


        tab1, tab2 = st.tabs(["Speaking Test", "Writing Test"])

        with tab1:

            cols = st.columns(2)
            for i, (topic, topic_info) in enumerate(speaking_topic_to_topic_info_map.items()):
                with cols[i % 2]:  # This will alternate between the two columns
                    display_topic(topic, topic_info, i)
        
        with tab2:
            cols = st.columns(2)
            for i, (topic, topic_info) in enumerate(writing_topic_to_topic_info_map.items()):
                with cols[i % 2]:  # This will alternate between the two columns
                    display_topic(topic, topic_info, i)


elif st.session_state["curr_page"] == "speaking__listen_and_answer":
    topic_info = speaking_topic_to_topic_info_map[st.session_state.curr_topic]
    st.title(topic_info['display_name'])

    # random í•˜ê²Œ ì§ˆë¬¸ í•˜ë‚˜ ê°€ì ¸ì˜¤ê¸°
    @st.cache_data
    def load_listen_and_answer_data():
        df = pd.read_csv("./data/speaking__listen_and_answer/question_and_audio.csv")
        return df

    df = load_listen_and_answer_data()

    if "question" not in st.session_state.exam_context:
        sample = df.sample(n=1).iloc[0]

        question = sample["question"]
        audio_file_path = sample["audio_file_path"]

        st.session_state.exam_context["sample"] = sample
        st.session_state.exam_context["question"] = question
        st.session_state.exam_context["audio_file_path"] = audio_file_path


    if st.button("Click to Start the Exam"):
        st.session_state.exam_context["exam_start"] = True
        st.session_state.exam_context["do_speech"] = True

    if st.session_state.exam_context.get("exam_start", False):
        if st.session_state.exam_context["do_speech"]:
            autoplay_audio(st.session_state.exam_context["audio_file_path"])
            st.session_state.exam_context["do_speech"] = False

        if not st.session_state.exam_context["do_speech"]:
            recognized_text = recognize_speech()
            st.session_state.exam_context["user_answer"] = recognized_text

        if st.session_state.exam_context.get("user_answer"):

            with st.container(border=True):
                answer_text = f"""
                - Question: {st.session_state.exam_context["question"]}
                - Your Answer: {st.session_state.exam_context.get("user_answer")}
                """

                st.markdown(answer_text)
                

            def get_speaking__listen_and_answer_result(answer_text):
                model = ChatOpenAI(model="gpt-4-1106-preview")
                class Score(BaseModel):
                    reason: str = Field(description="Questionì— ëŒ€í•´ Your Answerê°€ ì ì ˆí•œì§€ì— ëŒ€í•´ ì¶”ë¡ í•˜ë¼. ì˜ì–´ë¡œ.")
                    score: int = Field(description="Questionì— ëŒ€í•´ Your Answerê°€ ì ì ˆí•œì§€ì— ëŒ€í•´ 0~10ì  ì‚¬ì´ì˜ ì ìˆ˜ë¥¼ ë¶€ì—¬í•˜ë¼")
                parser = JsonOutputParser(pydantic_object=Score)
                format_instruction = parser.get_format_instructions()

                human_msg_prompt_template = HumanMessagePromptTemplate.from_template(
                    "{input}\n---\nQuestionì— ëŒ€í•´ Your Answerê°€ ì ì ˆí•œì§€ì— ëŒ€í•´ ì¶”ë¡ í•´ì„œ 0~10ì  ì‚¬ì´ì˜ ì ìˆ˜ë¥¼ ë¶€ì—¬í•´ë¼. ë‹¤ìŒì˜ í¬ë§·ì— ë§ì¶° ì‘ë‹µí•´ë¼.  : {format_instruction}")

                prompt_template = ChatPromptTemplate.from_messages(["human", human_msg_prompt_template])
                prompt_template = prompt_template.partial(format_instruction=format_instruction)
                    
                chain = prompt_template | model | parser
                return chain.invoke({"input": answer_text})
                
            with st.container(border=True):
                """
                ### Grading Result
                """

                with st.spinner("Grading..."):
                    result = get_speaking__listen_and_answer_result(answer_text)

                f"""
                {result['reason']}

                #### Grade: {result['score']}

                """

####################################
# 
elif st.session_state["curr_page"] == "speaking__express_an_opinion":
    topic_info = speaking_topic_to_topic_info_map[st.session_state.curr_topic]
    st.title(topic_info['display_name'])

    # random í•˜ê²Œ ì§ˆë¬¸ í•˜ë‚˜ ê°€ì ¸ì˜¤ê¸°
    @st.cache_data
    def load_speaking__express_an_opinion_data():
        df = pd.read_csv("./data/speaking__express_an_opinion/question_and_audio.csv")
        return df

    df = load_speaking__express_an_opinion_data()

    if "question" not in st.session_state.exam_context:
        sample = df.sample(n=1).iloc[0]

        question = sample["question"]
        audio_file_path = sample["audio_file_path"]

        st.session_state.exam_context["sample"] = sample
        st.session_state.exam_context["question"] = question
        st.session_state.exam_context["audio_file_path"] = audio_file_path


    if st.button("Click to Start the Exam"):
        st.session_state.exam_context["exam_start"] = True
        st.session_state.exam_context["do_speech"] = True

    if st.session_state.exam_context.get("exam_start", False):
        if st.session_state.exam_context["do_speech"]:
            autoplay_audio(st.session_state.exam_context["audio_file_path"])
            st.session_state.exam_context["do_speech"] = False

        if not st.session_state.exam_context["do_speech"]:
            recognized_text = recognize_speech()
            st.session_state.exam_context["user_answer"] = recognized_text

        if st.session_state.exam_context.get("user_answer"):

            with st.container(border=True):
                answer_text = f"""
                - Question: {st.session_state.exam_context["question"]}
                - Your Answer: {st.session_state.exam_context.get("user_answer")}
                """

                st.markdown(answer_text)
                
            with st.container(border=True):
                def get_speaking__express_opinion_result(answer_text):
                    model = ChatOpenAI(model="gpt-4-1106-preview")
                    class Score(BaseModel):
                        reason: str = Field(description="Questionì— ëŒ€í•´ ì˜ê²¬ì„ ë§í•˜ëŠ” ì‹œí—˜ì´ë‹¤. ì˜ê²¬ì„ ì ì ˆíˆ êµ¬ì¡°ì ìœ¼ë¡œ ì‘ë‹µí–ˆëŠ”ì§€ ì¶”ë¡ í•˜ë¼. ì˜ì–´ë¡œ.")
                        score: int = Field(description="Questionì— ëŒ€í•´ Your Answerê°€ ì¶©ë¶„íˆ ë…¼ë¦¬ì ìœ¼ë¡œ ì˜ê²¬ì„ í‘œí˜„í–ˆëŠ”ì§€ì— ëŒ€í•´ 0~10ì  ì‚¬ì´ì˜ ì ìˆ˜ë¥¼ ë¶€ì—¬í•˜ë¼.")
                    parser = JsonOutputParser(pydantic_object=Score)
                    format_instruction = parser.get_format_instructions()

                    human_msg_prompt_template = HumanMessagePromptTemplate.from_template(
                        "{input}\n---\nQuestionì— ëŒ€í•´ Your Answerê°€ ì¶©ë¶„íˆ ë…¼ë¦¬ì ìœ¼ë¡œ ì˜ê²¬ì„ í‘œí˜„í–ˆëŠ”ì§€ì— ëŒ€í•´ 0~10ì  ì‚¬ì´ì˜ ì ìˆ˜ë¥¼ ë¶€ì—¬í•˜ë¼. ë‹¤ìŒì˜ í¬ë§·ì— ë§ì¶° ì‘ë‹µí•´ë¼.  : {format_instruction}")
                    
                    prompt_template = ChatPromptTemplate.from_messages(["human", human_msg_prompt_template])
                    prompt_template = prompt_template.partial(format_instruction=format_instruction)
                    
                    chain = prompt_template | model | parser
                    return chain.invoke({"input": answer_text})

                """
                ### Grading Result
                """

                with st.spinner("Grading..."):
                    result = get_speaking__express_opinion_result(answer_text)

                f"""
                {result['reason']}

                #### Grade: {result['score']}

                """


elif st.session_state["curr_page"] == "speaking__debate":

    st.title("Discussion")

    con1 = st.container()
    con2 = st.container()

    user_input = ""

    if "model" not in st.session_state.exam_context:
        st.session_state.exam_context["model"] = ChatOpenAI(model="gpt-3.5-turbo")

    if "messages" not in st.session_state.exam_context:
        system_prompt = """\
- ë„ˆëŠ” AI ì‹œí—˜ ê°ë…ì´ë‹¤.
- userì˜ ì˜ì–´ ì‹¤ë ¥ì„ ìœ„í•´ ì–´ë– í•œ ì£¼ì œì— ëŒ€í•´ ì„œë¡œ ì§ˆë¬¸ê³¼ ë‹µì„í•˜ë©° í† ë¡ í•œë‹¤."""

        model = st.session_state.exam_context["model"]
        question = model.invoke("Create a controversial question for me.").content

        st.session_state.exam_context["messages"] = [SystemMessage(content=system_prompt),
                                                     AIMessage(content=question),
                                                     ]

        speech_file_path = "tmp_speak.mp3"
        response = client.audio.speech.create(
            model="tts-1",
            voice="alloy", # alloy, echo, fable, onyx, nova, and shimmer
            input=question
        )
        response.stream_to_file(speech_file_path)
        autoplay_audio(speech_file_path)

    with con1:
        for message in st.session_state.exam_context['messages']:
            if isinstance(message, SystemMessage):
                continue
            role = 'user' if message.type == 'human' else 'assistant'
            with st.chat_message(role):
                st.markdown(message.content)

    with con2:
        user_input = recognize_speech()

    with con1:
    
        turn_len = len(st.session_state.exam_context['messages'])
        max_turn_len = 5

        if user_input and turn_len < max_turn_len:
            st.session_state.exam_context['messages'].append(HumanMessage(content=user_input))

            with st.chat_message("user"):
                st.markdown(user_input)
            
            with st.chat_message("assistant"):
                message_placeholder = st.empty()
                full_response = ""

                model = st.session_state.exam_context["model"]

                for chunk in model.stream(st.session_state.exam_context['messages']):
                    full_response += (chunk.content or "")
                    message_placeholder.markdown(full_response + "â–Œ")
                message_placeholder.markdown(full_response)

                speech_file_path = "tmp_speak.mp3"
                response = client.audio.speech.create(
                model="tts-1",
                voice="alloy", # alloy, echo, fable, onyx, nova, and shimmer
                input=full_response
                )
                response.stream_to_file(speech_file_path)

                autoplay_audio(speech_file_path)

            st.session_state.exam_context['messages'].append(AIMessage(content=full_response))

        if turn_len >= max_turn_len:

            def get_speaking__debate_result(conversation):
                model = ChatOpenAI(model="gpt-4-1106-preview")
                class Score(BaseModel):
                    reason: str = Field(description="ì£¼ì–´ì§„ ëŒ€í™”ì— ëŒ€í•´ Userê°€ ì–¼ë§ˆë‚˜ ë…¼ë¦¬ì ì´ê³  ìœ ì°½í•˜ê²Œ ì˜ì–´ë¡œ ì‘ë‹µí•˜ì˜€ëŠ”ì§€ ì¶”ë¡ í•˜ë¼. ì˜ì–´ë¡œ.")
                    score: int = Field(description="ì£¼ì–´ì§„ ëŒ€í™”ì—ì„œ Userì˜ ì‘ë‹µì— ëŒ€í•´ ìœ ì°½ì„±ê³¼ ë…¼ë¦¬ì„±ì„ ê³ ë ¤í•˜ì—¬ 0~10ì  ì‚¬ì´ì˜ ì ìˆ˜ë¥¼ ë¶€ì—¬í•˜ë¼.")
                parser = JsonOutputParser(pydantic_object=Score)
                format_instruction = parser.get_format_instructions()

                human_msg_prompt_template = HumanMessagePromptTemplate.from_template(
                    "{input}\n---\nì£¼ì–´ì§„ ëŒ€í™”ì—ì„œ Userì˜ ì‘ë‹µì— ëŒ€í•´ ìœ ì°½ì„±ê³¼ ë…¼ë¦¬ì„±ì„ ê³ ë ¤í•˜ì—¬ 0~10ì  ì‚¬ì´ì˜ ì ìˆ˜ë¥¼ ë¶€ì—¬í•˜ë¼. ë‹¤ìŒì˜ í¬ë§·ì— ë§ì¶° ì‘ë‹µí•´ë¼.  : {format_instruction}")
                prompt_template = ChatPromptTemplate.from_messages(["human", human_msg_prompt_template])
                prompt_template = prompt_template.partial(format_instruction=format_instruction)
                    
                chain = prompt_template | model | parser
                return chain.invoke({"input": conversation})

                
            with st.container(border=True):
                """
                ### Grading Result
                """

                with st.spinner("Grading..."):

                    conversation = ""
                    for msg in st.session_state.exam_context["messages"]:
                        role = 'User' if msg.type == 'human' else 'AI'
                        conversation += f"{role}: {msg.content}"

                    result = get_speaking__debate_result(conversation)

                grade = ""

                if result['score'] >= 8:
                    grade = "Advanced"
                elif 4 < result['score'] < 8:
                    grade = "Intermediate"
                elif result['score'] <= 4:
                    grade = "Beginner"

                grade = f"{grade}, {result['score']}"

                f"""
                {result['reason']}

                #### Grade: {grade}
                """


elif  st.session_state["curr_page"] == "speaking__describe_img":
    topic_info = speaking_topic_to_topic_info_map[st.session_state.curr_topic]
    st.title(topic_info['display_name'])

    # random í•˜ê²Œ ì§ˆë¬¸ í•˜ë‚˜ ê°€ì ¸ì˜¤ê¸°
    @st.cache_data
    def load_speaking__describe_img():
        df = pd.read_csv("./data/speaking__describe_img/desc_img.csv")
        return df

    df = load_speaking__describe_img()

    if "img_path" not in st.session_state.exam_context:
        sample = df.sample(n=1).iloc[0]

        img_path = sample["img_path"]
        desc = sample["desc"]

        st.session_state.exam_context["img_path"] = img_path
        st.session_state.exam_context["desc"] = desc
        st.session_state.exam_context["recognized_text"] = ""

    st.image(st.session_state.exam_context['img_path'])
    
    with st.container(border=True):
        recognized_text = recognize_speech()
        if recognized_text:
            st.session_state.exam_context["recognized_text"] = recognized_text
        st.write(st.session_state.exam_context["recognized_text"])

    submit = st.button("Submit")

    if submit:
        def get_speaking__describe_img(user_input, ref):
            model = ChatOpenAI(model="gpt-4-1106-preview", temperature=0.8) # CoT ëŠ” ë‹¤ì–‘í•œ ìƒ˜í”Œì„ ë§Œë“¤ì–´ì•¼í•˜ê¸° ë•Œë¬¸ì— temperatureë¥¼ ì˜¬ë ¤ì•¼í•¨
            class Evaluation(BaseModel):
                score: int = Field(description="ì‚¬ì§„ ë¬˜ì‚¬í•˜ê¸° í‘œí˜„ í‘œí˜„ ì ìˆ˜. 0~10ì ")
                feedback: str = Field(description="ì‚¬ì§„ ë¬˜ì‚¬í•˜ê¸°ë¥¼ ë” ì˜ í•  ìˆ˜ ìˆë„ë¡í•˜ëŠ” ìì„¸í•œ í”¼ë“œë°±. Markdowní˜•ì‹, ì˜ì–´ë¡œ.")
            parser = JsonOutputParser(pydantic_object=Evaluation)
            format_instructions = parser.get_format_instructions()

            human_prompt_template = HumanMessagePromptTemplate.from_template(
            "ì‚¬ì§„ ë¬˜ì‚¬í•˜ê¸° ì˜ì–´ ì‹œí—˜ì´ë‹¤. ì‚¬ìš©ìì˜ ì‘ë‹µì„ Referenceì™€ ë¹„êµí•˜ì—¬ í‰ê°€í•˜ë¼.\nì‚¬ìš©ì: {input}\nReference: {ref}\n{format_instructions}")

            prompt = ChatPromptTemplate.from_messages([human_prompt_template])
            prompt = prompt.partial(format_instructions=format_instructions)
            eval_chain = prompt | model | parser

            result = eval_chain.invoke({"input": user_input, "ref": ref})
            return result


        st.title("Result & Feedback - Describing a Photo")

        with st.spinner("Generating Result & Feedback..."):

            result = get_speaking__describe_img(user_input=recognized_text,
                                                ref=st.session_state.exam_context['desc'])
        
            grade = ""
            if result['score'] >= 8:
                grade = "Advanced"
            elif 4 < result['score'] < 8:
                grade = "Intermediate"
            elif result['score'] <= 4:
                grade = "Beginner"

            score = f"{grade} ({result['score']}/10)"

            f"""
            Your response is a good starting approach at the `{score}` level for describing a given photo.

            Here are some feedback points.

            {result['feedback']}
            """


elif  st.session_state["curr_page"] == "speaking__describe_charts":
    topic_info = speaking_topic_to_topic_info_map[st.session_state.curr_topic]
    st.title(topic_info['display_name'])

    # random í•˜ê²Œ ì§ˆë¬¸ í•˜ë‚˜ ê°€ì ¸ì˜¤ê¸°
    @st.cache_data
    def load_speaking__describe_charts():
        df = pd.read_csv("./data/speaking__describe_charts/desc_charts.csv")
        return df

    df = load_speaking__describe_charts()

    if "img_path" not in st.session_state.exam_context:
        sample = df.sample(n=1).iloc[0]

        img_path = sample["img_path"]
        desc = sample["desc"]

        st.session_state.exam_context["img_path"] = img_path
        st.session_state.exam_context["desc"] = desc
        st.session_state.exam_context["recognized_text"] = ""

    st.image(st.session_state.exam_context['img_path'])
    
    with st.container(border=True):
        recognized_text = recognize_speech()
        if recognized_text:
            st.session_state.exam_context["recognized_text"] = recognized_text
        st.write(st.session_state.exam_context["recognized_text"])

    submit = st.button("ì œì¶œí•˜ê¸°")

    if submit:
        def get_speaking__describe_img(user_input, ref):
            model = ChatOpenAI(model="gpt-4-1106-preview", temperature=0.8) # CoT ëŠ” ë‹¤ì–‘í•œ ìƒ˜í”Œì„ ë§Œë“¤ì–´ì•¼í•˜ê¸° ë•Œë¬¸ì— temperatureë¥¼ ì˜¬ë ¤ì•¼í•¨
            class Evaluation(BaseModel):
                score: int = Field(description="ë„í‘œ ë³´ê³  ë°œí‘œí•˜ê¸° ì ìˆ˜. 0~10ì ")
                feedback: str = Field(description="ë„í‘œ ë³´ê³  ë°œí‘œí•˜ê¸° ì ìˆ˜. Markdowní˜•ì‹, ì˜ì–´ë¡œ.")
            parser = JsonOutputParser(pydantic_object=Evaluation)
            format_instructions = parser.get_format_instructions()

            human_prompt_template = HumanMessagePromptTemplate.from_template(
                            "ë„í‘œë³´ê³  ë°œí‘œí•˜ê¸° ì˜ì–´ ì‹œí—˜ì´ë‹¤. ì‚¬ìš©ìì˜ ì‘ë‹µì„ Referenceì™€ ë¹„êµí•˜ì—¬ í‰ê°€í•˜ë¼.\nì‚¬ìš©ì: {input}\Reference: {ref}\n{format_instructions}")

            prompt = ChatPromptTemplate.from_messages([human_prompt_template])
            prompt = prompt.partial(format_instructions=format_instructions)
            eval_chain = prompt | model | parser

            result = eval_chain.invoke({"input": user_input, "ref": ref})
            return result


        st.title("Result & Feedback- Describing a Chart")

        with st.spinner("Generating Result & Feedback..."):

            result = get_speaking__describe_img(user_input=recognized_text,
                                                ref=st.session_state.exam_context['desc'])
        
            grade = ""
            if result['score'] >= 8:
                grade = "Advanced"
            elif 4 < result['score'] < 8:
                grade = "Intermediate"
            elif result['score'] <= 4:
                grade = "Beginner"
            grade = f"{grade} ({result['score']}/10)"

            f"""
            Your response is a good starting approach at the `{grade}` level for describing a given photo.

            Here are some feedback points.

            {result['feedback']}
            """


elif  st.session_state["curr_page"] == "writing__dictation":
    from utils import grade_dictation

    topic_info = writing_topic_to_topic_info_map[st.session_state.curr_topic]
    st.title(topic_info['display_name'])

    # random í•˜ê²Œ ì§ˆë¬¸ í•˜ë‚˜ ê°€ì ¸ì˜¤ê¸°
    @st.cache_data
    def load_writing__dictation():
        df = pd.read_csv("./data/writing__dictation/sent_and_audio.csv")
        return df

    df = load_writing__dictation()

    if "sentence" not in st.session_state.exam_context:
        sample = df.sample(n=1).iloc[0]

        sentence = sample["sentence"]
        audio_file_path = sample["audio_file_path"]

        st.session_state.exam_context["sample"] = sample
        st.session_state.exam_context["sentence"] = sentence
        st.session_state.exam_context["audio_file_path"] = audio_file_path


    if st.button("Click to Start the Exam"):
        st.session_state.exam_context["exam_start"] = True
        st.session_state.exam_context["do_speech"] = True

    if st.session_state.exam_context.get("exam_start", False):
        if st.session_state.exam_context["do_speech"]:
            autoplay_audio(st.session_state.exam_context["audio_file_path"])
            st.session_state.exam_context["do_speech"] = False


        user_answer = st.text_input("user answer")
        if user_answer:
            st.session_state.exam_context["user_answer"] = user_answer

        if st.session_state.exam_context.get("user_answer"):

            with st.container(border=True):
                answer_text = f"""
                - Original sentence: {st.session_state.exam_context["sentence"]}
                - Your Answer: {st.session_state.exam_context.get("user_answer")}
                """

                st.markdown(answer_text)
                

            def get_writing__dictation_result(answer_text, ref):
                model = ChatOpenAI(model="gpt-4-1106-preview")
                class Evaluation(BaseModel):
                    reason: str = Field(description="ë°›ì•„ì“°ê¸° í‰ê°€ë¥¼ ìœ„í•œ ì¶”ë¡ . ì˜ì–´ë¡œ")
                    score: int = Field(description="ë°›ì•„ì“°ê¸° ì ìˆ˜. 0~10ì ")
                parser = JsonOutputParser(pydantic_object=Evaluation)
                format_instruction = parser.get_format_instructions()

                human_prompt_template = HumanMessagePromptTemplate.from_template(
                "ì˜ì–´ ë°›ì•„ì“°ê¸° ì‹œí—˜ì´ë‹¤. ì‚¬ìš©ìì˜ ì‘ë‹µì„ Referenceì™€ ë¹„êµí•˜ì—¬ í‰ê°€í•˜ë¼. \nì‚¬ìš©ì: {input}\nReference: {ref}\n{format_instructions}")

                # Create the prompt template from the human prompt template
                prompt_template = ChatPromptTemplate.from_messages([human_prompt_template])

                # Use the .partial() method correctly with the variable format_instructions
                prompt_template = prompt_template.partial(format_instructions=format_instruction)

                chain = prompt_template | model | parser
                
                return chain.invoke({"input": answer_text, "ref": ref})
                
            with st.container(border=True):
                """
                ### Grading Result
                """

                with st.spinner("Grading..."):
                    model_result = get_writing__dictation_result(answer_text, st.session_state.exam_context['sentence'])
                    automatic_result = grade_dictation(correct_script=st.session_state.exam_context['sentence'], student_response=answer_text)

                    model_score = model_result['score']
                    automatic_score = automatic_result['accuracy'] * 10 # between 0 and 1. So multiply by 10
                    final_score = (model_score + automatic_score)/2

                f"""
                ### Evaluation Results from the Model 
                {model_result['reason']}
                Grade: {model_score}

                ### Evaluation Results from the Levenshtein Distance
                Grade: {automatic_score}

                #### Total Grade: {final_score}

                """

